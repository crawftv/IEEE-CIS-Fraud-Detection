{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this kernel\n",
    "\n",
    "Before I get started, I just wanted to say: huge props to Inversion! The official starter kernel is **AWESOME**; it's so simple, clean, straightforward, and pragmatic. It certainly saved me a lot of time wrangling with data, so that I can directly start tuning my models (real data scientists will call me lazy, but hey I'm an engineer I just want my stuff to work).\n",
    "\n",
    "I noticed two tiny problems with it:\n",
    "* It takes a lot of RAM to run, which means that if you are using a GPU, it might crash as you try to fill missing values.\n",
    "* It takes a while to run (roughly 3500 seconds, which is more than an hour; again, I'm a lazy guy and I don't like waiting).\n",
    "\n",
    "With this kernel, I bring some small changes:\n",
    "* Decrease RAM usage, so that it won't crash when you change it to GPU. I simply changed when we are deleting unused variables.\n",
    "* Decrease **running time from ~3500s to ~40s** (yes, that's almost 90x faster), at the cost of a slight decrease in score. This is done by adding a single argument.\n",
    "\n",
    "Again, my changes are super minimal (cause Inversion's kernel was already so awesome), but I hope it will save you some time and trouble (so that you can start working on cool stuff).\n",
    "\n",
    "\n",
    "### Changelog\n",
    "\n",
    "**V4**\n",
    "* Change some wording\n",
    "* Prints XGBoost version\n",
    "* Add random state to XGB for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "print(\"XGBoost version:\", xgb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Preprocessing\n",
    "\n",
    "This preprocessing method is more careful with RAM usage, which avoids crashing the kernel when you switch from CPU to GPU. Otherwise, it is exactly the same procedure as the official starter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\n",
    "test_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n",
    "\n",
    "train_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\n",
    "test_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n",
    "\n",
    "sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')\n",
    "\n",
    "train = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
    "test = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "y_train = train['isFraud'].copy()\n",
    "del train_transaction, train_identity, test_transaction, test_identity\n",
    "\n",
    "# Drop target, fill in NaNs\n",
    "X_train = train.drop('isFraud', axis=1)\n",
    "X_test = test.copy()\n",
    "\n",
    "del train, test\n",
    "\n",
    "X_train = X_train.fillna(-999)\n",
    "X_test = X_test.fillna(-999)\n",
    "\n",
    "# Label Encoding\n",
    "for f in X_train.columns:\n",
    "    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n",
    "        X_train[f] = lbl.transform(list(X_train[f].values))\n",
    "        X_test[f] = lbl.transform(list(X_test[f].values))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "To activate GPU usage, simply use `tree_method='gpu_hist'` (took me an hour to figure out, I wish XGBoost documentation was clearer about that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    missing=-999,\n",
    "    random_state=2019,\n",
    "    tree_method='gpu_exact'  # THE MAGICAL PARAMETER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of you must be wondering how we were able to decrease the fitting time by that much. The reason for that is not only we are running on gpu, but we are also computing an approximation of the real underlying algorithm (which is a greedy algorithm). This hurts your score slightly, but as a result is much faster.\n",
    "\n",
    "So why am I not using CPU with `tree_method='hist'`? If you try it out yourself, you'll realize it'll take ~ 7 min, which is still far from the GPU fitting time. Similarly, `tree_method='gpu_exact'` will take ~ 4 min, but likely yields better accuracy than `gpu_hist` or `hist`.\n",
    "\n",
    "The [docs on parameters](https://xgboost.readthedocs.io/en/latest/parameter.html) has a section on `tree_method`, and it goes over the details of each option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['isFraud'] = clf.predict_proba(X_test)[:,1]\n",
    "sample_submission.to_csv('simple_xgboost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stupid XGBoost\n",
    "-with no hyper parameter tuning at all XGBoost produces a success rate of .938%. To get to 1st place I need to improve 1.1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size = .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=9,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    missing=-999,\n",
    "    random_state=2019,\n",
    "    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = clf.predict_proba(X_val)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "IMPORTANT must upgrade Seaborn to use in google Colab.\n",
    "Classification_report is just the sklearn classification report\n",
    "Classification_report will show up in the shell and notebooks\n",
    "Results from confusion_viz will appear in notebooks only\n",
    "\"\"\"\n",
    "def classification_visualization(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Prints the results of the functions. That's it\n",
    "    \"\"\"\n",
    "    print(classification_report(y_true,y_pred))\n",
    "    print(confusion_viz(y_true,y_pred))\n",
    "def confusion_viz(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Uses labels as given\n",
    "    Pass y_true,y_pred, same as any sklearn classification problem\n",
    "    Inspired from code from a Ryan Herr Lambda School Lecture\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true).ravel()\n",
    "    labels = unique_labels(y_true,y_pred)\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    graph = sns.heatmap(matrix, annot=True,\n",
    "                       fmt=',', linewidths=1,linecolor='grey',\n",
    "                       square=True,\n",
    "                       xticklabels=[\"Predicted\\n\" + str(i) for i in labels],\n",
    "                       yticklabels=[\"Actual\\n\" + str(i) for i in labels],\n",
    "                       robust=True,\n",
    "                       cmap=sns.color_palette(\"coolwarm\"))\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=0)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_visualization(y_val,y_val_pred.round() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = precision_recall_curve(y_val, y_val_pred)\n",
    "plt.plot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "def plt_prc(y_true, y_pred):\n",
    "    a,b,c  = precision_recall_curve(y_true,y_pred)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(a, b, color='darkorange',\n",
    "             lw=lw, label='Precision Recall curve')#(area = %0.2f)' % c)\n",
    "    plt.plot([0, 1], [1, 0], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "plt_prc(y_val,y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this XGBoost model with the the parameter from notebook i forked did a good job of classifying the 0's, but less than fifty % fo a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision_score(y_val,y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stupid hyper parameter tuning.\n",
    "Lets just try some hyper parameter tuning with no data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gbrt_minimize, gp_minimize\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.space import Real, Categorical, Integer  \n",
    "\n",
    "dim_n_estimators = Integer(low=1e2, high=1e3, name='n_estimators')\n",
    "dim_max_depth = Integer(low=1e0, high =2e1,name='max_depth')\n",
    "dim_learning_rate = Real(1e-3,1e-1, name=\"learning_rate\")\n",
    "\n",
    "dimensions = [dim_n_estimators, dim_max_depth, dim_learning_rate]\n",
    "\n",
    "default_parameters = [500, 9, 0.05]\n",
    "\n",
    "def create_model(n_estimators, max_depth, learning_rate):\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        missing=-999,\n",
    "        random_state=2019,\n",
    "        tree_method='gpu_hist'\n",
    "    )\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_named_args(dimensions=dimensions)\n",
    "def fitness(n_estimators, max_depth, learning_rate):\n",
    "    model = create_model(n_estimators = n_estimators, max_depth=max_depth, learning_rate=learning_rate)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_val_pred = clf.predict_proba(X_val)[:,1]\n",
    "    score = average_precision_score(y_val,y_val_pred)\n",
    "    print(\"Average_Precision_Score = %f\" %score)\n",
    "    \n",
    "    del model\n",
    "    \n",
    "    return -score\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_result = gp_minimize(func=fitness,\n",
    "                            dimensions=dimensions,\n",
    "                            n_calls=12,\n",
    "                            noise= 0.01,\n",
    "                            n_jobs=-1,\n",
    "                            kappa = 5,\n",
    "                            x0=default_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
